% esempio citazione articolo
@article{articolo1,
  author		= "Slifka, M. K. and Whitton, J. L.",
  title			= "Clinical implications of dysregulated cytokine production",
  journal		= "J. {M}ol. {M}ed.",
  volume		= "78",
  pages			= "74--80",
  year			= "2000",
  doi			= "10.1007/s001090000086"
}

% esempio citazione sito web
@misc{sito1,
  title        = {CiteDrive brings reference management to Overleaf},
  author       = {CiteDrive, Inc},
  year         = 2022,
  note = {\url{https://www.elsevier.com/}}
}
@misc{hashTrick,
  title  = {Feature hashing},
  author = {wikipedia},
  
  note   = {\url{https://en.wikipedia.org/wiki/Feature_hashing}, Visitato il 3 Novembre 2025 }
}
@misc{saxe2015deepneuralnetworkbased,
  title         = {Deep Neural Network Based Malware Detection Using Two Dimensional Binary Program Features},
  author        = {Joshua Saxe and Konstantin Berlin},
  year          = {2015},
  eprint        = {1508.03096},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CR},
  url           = {https://arxiv.org/abs/1508.03096}
}
% esempio citazione libro
@book{libro1,
  author		= "Geddes, K. O. and Czapor, S. R. and Labahn, G.",
  title			= "Algorithms for {C}omputer {A}lgebra",
  address		= "Boston",
  publisher		= "Kluwer",
  year			= "1992"
}

@misc{kaspersky2024maliciousfiles,
  author      = {Kaspersky},
  title       = {The Cyber Surge: Kaspersky Detected 467{,}000 Malicious Files Daily in 2024},
  year        = {2025},
  month       = {March},
  howpublished= {Press Release},
  url         = {https://www.kaspersky.com/about/press-releases/the-cyber-surge-kaspersky-detected-467000-malicious-files-daily-in-2024},
  note        = {Visitato il 23 October 2025}
}

@misc{PeFormatDocs,
  author = {Microsoft},
  
  title = {PE Format},
  year = 2025,
  note = {https://learn.microsoft.com/en-us/windows/win32/debug/pe-format},
}

@misc{MZRSTpeStructure,
  author = {MZRST},
  title = {PE File Format Structure},
  howpublished = {\url{https://www.mzrst.com/}},
}

@misc{deepAImlGlossary,
  author = {DeepAI},
  title = {Machine Learning Glossary and Terms},
  howpublished = {\url{https://deepai.org/machine-learning-glossary-and-terms/machine-learning}},
  note = {Visitato il 4 Novembre 2025}
}

@misc{LIEF,
  author = {Quarkslab},
  title = {LIEF - Library to Instrument Executable Formats},
  year = {2024},
  howpublished = {\url{https://lief-project.github.io/}},
}
@article{olivander,
  author  = {De Rose, Luca and Andresini, Giuseppina and Appice, Annalisa and Malerba, Donato},
  year    = {2025},
  month   = {07},
  pages   = {},
  title   = {OLIVANDER: a counterfactual-based method to generate adversarial Windows PE malware},
  volume  = {39},
  journal = {Data Mining and Knowledge Discovery},
  doi     = {10.1007/s10618-025-01127-1}
}
@article{9437194,
  author   = {Demetrio, Luca and Biggio, Battista and Lagorio, Giovanni and Roli, Fabio and Armando, Alessandro},
  journal  = {IEEE Transactions on Information Forensics and Security},
  title    = {Functionality-Preserving Black-Box Optimization of Adversarial Windows Malware},
  year     = {2021},
  volume   = {16},
  number   = {},
  pages    = {3469-3478},
  keywords = {Malware;Detectors;Optimization;Operating systems;Feature extraction;Payloads;Minimization;Adversarial examples;malware detection;evasion attacks;black-box optimization;machine learning},
  doi      = {10.1109/TIFS.2021.3082330}
}

H. Anderson and P. Roth, "EMBER: An Open Dataset for Training Static PE Malware Machine Learning Models”, in ArXiv e-prints. Apr. 2018.

@article{2018arXiv180404637A,
  author        = {{Anderson}, H.~S. and {Roth}, P.},
  title         = {{EMBER: An Open Dataset for Training Static PE Malware Machine Learning Models}},
  journal       = {ArXiv e-prints},
  archiveprefix = {arXiv},
  eprint        = {1804.04637},
  primaryclass  = {cs.CR},
  keywords      = {Computer Science - Cryptography and Security},
  year          = 2018,
  month         = apr,
  adsurl        = {http://adsabs.harvard.edu/abs/2018arXiv180404637A}
}

@inproceedings{Joyce_2025,
  series     = {KDD ’25},
  title      = {EMBER2024 - A Benchmark Dataset for Holistic Evaluation of Malware Classifiers},
  url        = {http://dx.doi.org/10.1145/3711896.3737431},
  doi        = {10.1145/3711896.3737431},
  booktitle  = {Proceedings of the 31st ACM SIGKDD Conference on Knowledge Discovery and Data Mining V.2},
  publisher  = {ACM},
  author     = {Joyce, Robert J. and Miller, Gideon and Roth, Phil and Zak, Richard and Zaresky-Williams, Elliott and Anderson, Hyrum and Raff, Edward and Holt, James},
  year       = {2025},
  month      = aug,
  pages      = {5516–5526},
  collection = {KDD ’25}
}
@inproceedings{10.1145/1128817.1128824,
  author    = {Barreno, Marco and Nelson, Blaine and Sears, Russell and Joseph, Anthony D. and Tygar, J. D.},
  title     = {Can machine learning be secure?},
  year      = {2006},
  isbn      = {1595932720},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/1128817.1128824},
  doi       = {10.1145/1128817.1128824},
  abstract  = {Machine learning systems offer unparalled flexibility in dealing with evolving input in a variety of applications, such as intrusion detection systems and spam e-mail filtering. However, machine learning algorithms themselves can be a target of attack by a malicious adversary. This paper provides a framework for answering the question, "Can machine learning be secure?" Novel contributions of this paper include a taxonomy of different types of attacks on machine learning techniques and systems, a variety of defenses against those attacks, a discussion of ideas that are important to security for machine learning, an analytical model giving a lower bound on attacker's work function, and a list of open problems.},
  booktitle = {Proceedings of the 2006 ACM Symposium on Information, Computer and Communications Security},
  pages     = {16–25},
  numpages  = {10},
  keywords  = {statistical learning, spam filters, security metrics, machine learning, intrusion detection, game theory, computer security, computer networks, adversarial learning},
  location  = {Taipei, Taiwan},
  series    = {ASIACCS '06}
}
@article{PITROPAKIS2019100199,
  title    = {A taxonomy and survey of attacks against machine learning},
  journal  = {Computer Science Review},
  volume   = {34},
  pages    = {100199},
  year     = {2019},
  issn     = {1574-0137},
  doi      = {https://doi.org/10.1016/j.cosrev.2019.100199},
  url      = {https://www.sciencedirect.com/science/article/pii/S1574013718303289},
  author   = {Nikolaos Pitropakis and Emmanouil Panaousis and Thanassis Giannetsos and Eleftherios Anastasiadis and George Loukas},
  keywords = {Machine learning, Attacks, Taxonomy, Survey},
  abstract = {The majority of machine learning methodologies operate with the assumption that their environment is benign. However, this assumption does not always hold, as it is often advantageous to adversaries to maliciously modify the training (poisoning attacks) or test data (evasion attacks). Such attacks can be catastrophic given the growth and the penetration of machine learning applications in society. Therefore, there is a need to secure machine learning enabling the safe adoption of it in adversarial cases, such as spam filtering, malware detection, and biometric recognition. This paper presents a taxonomy and survey of attacks against systems that use machine learning. It organizes the body of knowledge in adversarial machine learning so as to identify the aspects where researchers from different fields can contribute to. The taxonomy identifies attacks which share key characteristics and as such can potentially be addressed by the same defence approaches. Thus, the proposed taxonomy makes it easier to understand the existing attack landscape towards developing defence mechanisms, which are not investigated in this survey. The taxonomy is also leveraged to identify open problems that can lead to new research areas within the field of adversarial machine learning.}
}
@inproceedings{liu2017neural,
  title        = {Neural trojans},
  author       = {Liu, Yuntao and Xie, Yang and Srivastava, Ankur},
  booktitle    = {2017 IEEE international conference on computer design (ICCD)},
  pages        = {45--48},
  year         = {2017},
  organization = {IEEE}
}
@misc{huang2016learningstrongadversary,
  title         = {Learning with a Strong Adversary},
  author        = {Ruitong Huang and Bing Xu and Dale Schuurmans and Csaba Szepesvari},
  year          = {2016},
  eprint        = {1511.03034},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG},
  url           = {https://arxiv.org/abs/1511.03034}
}
@inproceedings{5360532,
  author    = {Liu, Wei and Chawla, Sanjay},
  booktitle = {2009 IEEE International Conference on Data Mining Workshops},
  title     = {A Game Theoretical Model for Adversarial Learning},
  year      = {2009},
  volume    = {},
  number    = {},
  pages     = {25-30},
  keywords  = {Game theory;Genetic algorithms;Information technology;Australia;Filters;Data mining;Conferences;Testing;Robustness;Polynomials},
  doi       = {10.1109/ICDMW.2009.9}
}
@article{10.1145/3473039,
  author     = {Demetrio, Luca and Coull, Scott E. and Biggio, Battista and Lagorio, Giovanni and Armando, Alessandro and Roli, Fabio},
  title      = {Adversarial EXEmples: A Survey and Experimental Evaluation of Practical Attacks on Machine Learning for Windows Malware Detection},
  year       = {2021},
  issue_date = {November 2021},
  publisher  = {Association for Computing Machinery},
  address    = {New York, NY, USA},
  volume     = {24},
  number     = {4},
  issn       = {2471-2566},
  url        = {https://doi.org/10.1145/3473039},
  doi        = {10.1145/3473039},
  abstract   = {Recent work has shown that adversarial Windows malware samples—referred to as adversarial EXEmples in this article—can bypass machine learning-based detection relying on static code analysis by perturbing relatively few input bytes. To preserve malicious functionality, previous attacks either add bytes to existing non-functional areas of the file, potentially limiting their effectiveness, or require running computationally demanding validation steps to discard malware variants that do not correctly execute in sandbox environments. In this work, we overcome these limitations by developing a unifying framework that does not only encompass and generalize previous attacks against machine-learning models, but also includes three novel attacks based on practical, functionality-preserving manipulations to the Windows Portable Executable file format. These attacks, named Full DOS, Extend, and Shift, inject the adversarial payload by respectively manipulating the DOS header, extending it, and shifting the content of the first section. Our experimental results show that these attacks outperform existing ones in both white-box and black-box scenarios, achieving a better tradeoff in terms of evasion rate and size of the injected payload, while also enabling evasion of models that have been shown to be robust to previous attacks. To facilitate reproducibility of our findings, we open source our framework and all the corresponding attack implementations as part of the secml-malware Python library. We conclude this work by discussing the limitations of current machine learning-based malware detectors, along with potential mitigation strategies based on embedding domain knowledge coming from subject-matter experts directly into the learning process.},
  journal    = {ACM Trans. Priv. Secur.},
  month      = sep,
  articleno  = {27},
  numpages   = {31},
  keywords   = {Adversarial examples, evasion, malware detection, semantics-invariant manipulations}
}
@misc{demetrio2024secmlmalwarepentestingwindowsmalware,
  title         = {secml-malware: Pentesting Windows Malware Classifiers with Adversarial EXEmples in Python},
  author        = {Luca Demetrio and Battista Biggio},
  year          = {2024},
  eprint        = {2104.12848},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CR},
  url           = {https://arxiv.org/abs/2104.12848}
}
@article{deap,
  author  = {Fortin, Félix-Antoine and De Rainville, François-Michel and Gardner, M.A. and Parizeau, Marc and Gagné, Christian},
  year    = {2012},
  month   = {07},
  pages   = {2171-2175},
  title   = {DEAP: Evolutionary algorithms made easy},
  volume  = {13},
  journal = {Journal of Machine Learning Research, Machine Learning Open Source Software}
}
@misc{evangelatos2025exploringenergylandscapesminimal,
  title         = {Exploring Energy Landscapes for Minimal Counterfactual Explanations: Applications in Cybersecurity and Beyond},
  author        = {Spyridon Evangelatos and Eleni Veroni and Vasilis Efthymiou and Christos Nikolopoulos and Georgios Th. Papadopoulos and Panagiotis Sarigiannidis},
  year          = {2025},
  eprint        = {2503.18185},
  archiveprefix = {arXiv},
  primaryclass  = {cs.AI},
  url           = {https://arxiv.org/abs/2503.18185}
}

@misc{mutualInformation_wikipedia,
  title        = {Mutual information},
  author       = {Wikipedia},
  year         = {2025},
  howpublished = {\url{https://en.wikipedia.org/wiki/Mutual_information}},
  note         = {Visitato il 7 Novembre 2025}
}
@misc{saini2024reviewdualityadversariallearning,
  title         = {A Review of the Duality of Adversarial Learning in Network Intrusion: Attacks and Countermeasures},
  author        = {Shalini Saini and Anitha Chennamaneni and Babatunde Sawyerr},
  year          = {2024},
  eprint        = {2412.13880},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CR},
  url           = {https://arxiv.org/abs/2412.13880}
}
@inproceedings{mothilal2020dice,
  title     = {Explaining machine learning classifiers through diverse counterfactual explanations},
  author    = {Mothilal, Ramaravind K and Sharma, Amit and Tan, Chenhao},
  booktitle = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
  pages     = {607--617},
  year      = {2020}
}
@misc{madry2019deeplearningmodelsresistant,
  title         = {Towards Deep Learning Models Resistant to Adversarial Attacks},
  author        = {Aleksander Madry and Aleksandar Makelov and Ludwig Schmidt and Dimitris Tsipras and Adrian Vladu},
  year          = {2019},
  eprint        = {1706.06083},
  archiveprefix = {arXiv},
  primaryclass  = {stat.ML},
  url           = {https://arxiv.org/abs/1706.06083}
}

@inproceedings{zhang2019trades,
  author    = {Hao Zhang and Pengfei Wang and Huan Zhang and Cho-Jui Hsieh and Dimitris Papailiopoulos},
  title     = {TRADES: A Tradeoff-inspired Adversarial Defense via Surrogate-loss Minimization},
  booktitle = {International Conference on Machine Learning (ICML)},
  year      = {2019},
  note      = {Theoretically principled trade-off between robustness and accuracy}
}

@article{cohen2019randomizedsmoothing,
  author  = {Jeremy M. Cohen and Elan Rosenfeld and J. Zico Kolter},
  title   = {Certified Adversarial Robustness via Randomized Smoothing},
  journal = {Journal of Machine Learning Research},
  year    = {2019},
  volume  = {https://arxiv.org/abs/1902.02918},
  note    = {published as arXiv:1902.02918}
}

@inproceedings{wong2018provable,
  author    = {Eric Wong and J. Zico Kolter},
  title     = {Provable Defenses against Adversarial Examples via the Convex Outer Adversarial Polytope},
  booktitle = {International Conference on Machine Learning (ICML)},
  year      = {2018}
}

@inproceedings{xie2019featuredenoising,
  author    = {Cihang Xie and Yinpeng Dong and Jingfeng Wu and Kun He and Abdulkadir Dener and Hao Li and Xiangyu Zhang and Jian Sun},
  title     = {Feature Denoising for Improving Adversarial Robustness},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  year      = {2019}
}
@misc{goodfellow2015explainingharnessingadversarialexamples,
  title         = {Explaining and Harnessing Adversarial Examples},
  author        = {Ian J. Goodfellow and Jonathon Shlens and Christian Szegedy},
  year          = {2015},
  eprint        = {1412.6572},
  archiveprefix = {arXiv},
  primaryclass  = {stat.ML},
  url           = {https://arxiv.org/abs/1412.6572}
}
@misc{wang2023adversarialattacksdefensesmachine,
  title         = {Adversarial Attacks and Defenses in Machine Learning-Powered Networks: A Contemporary Survey},
  author        = {Yulong Wang and Tong Sun and Shenghong Li and Xin Yuan and Wei Ni and Ekram Hossain and H. Vincent Poor},
  year          = {2023},
  eprint        = {2303.06302},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG},
  url           = {https://arxiv.org/abs/2303.06302}
}
@inproceedings{kurakin2017adversarial,
  title={Adversarial examples in the physical world},
  author={Kurakin, Alexey and Goodfellow, Ian and Bengio, Samy},
  booktitle={International Conference on Learning Representations},
  year={2017}
}

@inproceedings{madry2018towards,
  title={Towards deep learning models resistant to adversarial attacks},
  author={Madry, Aleksander and Makelov, Aleksandar and Schmidt, Ludwig and Tsipras, Dimitris and Vladu, Adrian},
  booktitle={International Conference on Learning Representations},
  year={2018}
}

@inproceedings{papernot2016limitations,
  title={The limitations of deep learning in adversarial settings},
  author={Papernot, Nicolas and McDaniel, Patrick and Jha, Somesh and Fredrikson, Matt and Celik, Z Berkay and Swami, Ananthram},
  booktitle={2016 IEEE European symposium on security and privacy (EuroS\&P)},
  pages={372--387},
  year={2016},
  organization={IEEE}
}

@misc{papernot2015distillation,
  author        = {Nicolas Papernot and Patrick McDaniel and Xi Wu and Somesh Jha and Ananthram Swami},
  title         = {Distillation as a Defense to Adversarial Perturbations against Deep Neural Networks},
  year          = {2015},
  eprint        = {1511.04508},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CR},
  url           = {https://arxiv.org/abs/1511.04508}
}

@inproceedings{tramer2018ensemble,
  author    = {Fabrice Tram{ e8r} and Nicolas Papernot and Ian Goodfellow and Dan Boneh and Patrick McDaniel},
  title     = {Ensemble Adversarial Training: Attacks and Defenses},
  booktitle = {Proceedings of the 6th International Conference on Learning Representations (ICLR) Workshop},
  year      = {2018},
  note      = {ArXiv:1705.07204}
}

@inproceedings{kannan2018advlogitpair,
  author    = {Hussam Kannan and Alexey Kurakin and Ian Goodfellow},
  title     = {Adversarial Logit Pairing},
  booktitle = {International Conference on Learning Representations (ICLR) Workshop},
  year      = {2018},
  note      = {ArXiv:1803.06373}
}

@misc{athalye2018obfuscated,
  author        = {Anish Athalye and Nicholas Carlini and David Wagner},
  title         = {Obfuscated Gradients Give a False Sense of Security: Circumventing Defenses to Adversarial Examples},
  year          = {2018},
  eprint        = {1802.00420},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CR}
}

@inproceedings{tramer2018ensembleclean,
  author    = {Fabrice Tramer and Nicolas Papernot and Ian Goodfellow and Dan Boneh and Patrick McDaniel},
  title     = {Ensemble Adversarial Training: Attacks and Defenses},
  booktitle = {International Conference on Learning Representations (ICLR) Workshop},
  year      = {2018},
  note      = {ArXiv:1705.07204}
}

@inproceedings{xu2017feature,
  author    = {Weilin Xu and David Evans and Yanjun Qi},
  title     = {Feature Squeezing: Detecting Adversarial Examples in Deep Neural Networks},
  booktitle = {Network and Distributed Systems Security Symposium (NDSS)},
  year      = {2018},
  note      = {arXiv:1704.01155}
}

@misc{ma2018characterizinglid,
  author        = {Xingjun Ma and Bo Li and Yisen Wang and Sarah M. Erfani and Sudanthi N.R. Wijewickrema and Grant Schoenebeck and Dawn Song and Michael E. Houle and James Bailey},
  title         = {Characterizing Adversarial Subspaces Using Local Intrinsic Dimensionality},
  year          = {2018},
  eprint        = {1801.02613},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG}
}

@inproceedings{metzen2017detecting,
  author    = {Jan Hendrik Metzen and Tim Genewein and Volker Fischer and Bastian Bischoff},
  title     = {On Detecting Adversarial Perturbations},
  booktitle = {International Conference on Learning Representations (ICLR) Workshop},
  year      = {2017}
}

% --- Poisoning / Backdoor literature (non-malware) ---
@misc{gu2017badnets,
  author = {Tianyu Gu and Brendan Dolan-Gavitt and Siddharth Garg},
  title  = {BadNets: Identifying Vulnerabilities in the Machine Learning Model Supply Chain},
  year   = {2017},
  eprint = {1708.06733},
  archiveprefix = {arXiv},
  primaryclass = {cs.CR}
}

@misc{chen2017targeted,
  author = {Xinyun Chen and Chang Liu and Bo Li and Kimberly Lu and Dawn Song},
  title  = {Targeted Backdoor Attacks on Deep Learning Systems Using Data Poisoning},
  year   = {2017},
  eprint = {1712.05526},
  archiveprefix = {arXiv},
  primaryclass = {cs.CR}
}

@inproceedings{shafahi2018poisonfrogs,
  author = {Ali Shafahi and W. Ronny Huang and Mahyar Najibi and Octavian Suciu and Christoph Studer and Tudor Dumitras and Tom Goldstein},
  title  = {Poison Frogs! Targeted Clean-Label Poisoning Attacks on Neural Networks},
  booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
  year   = {2018}
}

@article{koh2018stronger,
  author = {Pang Wei Koh and Jacob Steinhardt and Percy Liang},
  title  = {Stronger Data Poisoning Attacks Break Data Sanitization Defenses},
  year   = {2018},
  eprint = {1811.00741},
  archiveprefix = {arXiv},
  primaryclass = {stat.ML}
}

@misc{bagdasaryan2018federatedbackdoor,
  author = {Eugene Bagdasaryan and Andreas Veit and Yiqing Hua and Deborah Estrin and Vitaly Shmatikov},
  title  = {How To Backdoor Federated Learning},
  year   = {2018},
  eprint = {1807.00459},
  archiveprefix = {arXiv},
  primaryclass = {cs.CR}
}

@misc{fung2018foolsgold,
  author = {Clement Fung and Chris J. M. Yoon and Ivan Beschastnikh},
  title  = {Mitigating Sybils in Federated Learning Poisoning},
  year   = {2018},
  eprint = {1808.04866},
  archiveprefix = {arXiv},
  primaryclass = {cs.LG}
}

@inproceedings{huang2020metapoison,
  author = {W. Ronny Huang and Jonas Geiping and Liam Fowl and Gavin Taylor and Tom Goldstein},
  title  = {MetaPoison: Practical General-purpose Clean-label Data Poisoning},
  booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
  year   = {2020}
}

@inproceedings{biggio2012poisoning,
  author = {Battista Biggio and Blaine Nelson and Pavel Laskov},
  title  = {Poisoning Attacks against Support Vector Machines},
  booktitle = {Proceedings of the 29th International Conference on Machine Learning (ICML)},
  year   = {2012}
}

@misc{feinman2017detecting,
  author        = {Ryan Feinman and Ryan R. Curtin and Saurabh Shankar and Andrew C. Sharma and Ankit Raghunathan and Peter L. Bartlett and Patrick D. McDaniel},
  title         = {Detecting Adversarial Examples through Bayesian Uncertainty Estimation},
  year          = {2017},
  eprint        = {1703.00410},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG}
}

@inproceedings{meng2017magnet,
  author    = {Nan Meng and Huan Zhang and Maziar Sanjabi and Richard E. K. Dhaliwal and Somesh Jha and Aleksander Madry and Yanjun Qi and Suman Jana and Pin-Yu Chen and Huan Zhang},
  title     = {MagNet: A Two-Pronged Defense against Adversarial Examples},
  booktitle = {ACM Conference on Computer and Communications Security (CCS) Workshop},
  year      = {2017},
  note      = {original paper: Meng and Chen 2017}
}

@misc{carlini2017bypassing,
  author        = {Nicholas Carlini and David Wagner},
  title         = {Adversarial Examples Are Not Easily Detected: Bypassing Ten Detection Methods},
  year          = {2017},
  eprint        = {1705.07263},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG}
}

@inproceedings{zugner2018adversarial,
  author    = {Daniel Z{"u}gner and Stephan G{"u}nnemann},
  title     = {Adversarial Attacks on Graph Neural Networks: Perturbations and Detection},
  booktitle = {ACM Conference on Knowledge Discovery and Data Mining (KDD)},
  year      = {2018},
}

@article{grosse2017adversarial,
  author  = {Kathrin Grosse and Nicolas Papernot and Praveen Manoharan and Michael Backes and Patrick McDaniel},
  title   = {Adversarial Examples for Malware Detection},
  journal = {ArXiv e-prints},
  year    = {2017},
  eprint  = {1701.08941},
}

@inproceedings{chen2017zoo,
  author    = {Pin-Yu Chen and Huan Zhang and Yash Sharma and Jinfeng Yi and Cho-Jui Hsieh},
  title     = {ZOO: Zeroth Order Optimization based Black-box Attacks to Deep Neural Networks without Training Substitute Models},
  booktitle = {10th {ACM} Workshop on Artificial Intelligence and Security (AISec) / {CCS}},
  year      = {2017},
  url       = {https://arxiv.org/abs/1708.03999},
}

@article{bhagoji2017exploring,
  author    = {Arjun Nitin Bhagoji and Warren He and Bo Li and Dawn Song},
  title     = {Exploring the Space of Black-box Attacks on Deep Neural Networks},
  journal   = {arXiv e-prints},
  year      = {2017},
  url       = {https://arxiv.org/abs/1712.09491},
}

@article{chen2019hopskipjump,
  author    = {Jianbo Chen and Martin J. Wainwright and Michael I. Jordan and et al.},
  title     = {HopSkipJumpAttack: A Query-Efficient Decision-Based Attack},
  journal   = {arXiv e-prints},
  year      = {2019},
  url       = {https://arxiv.org/abs/1904.02144},
}
@misc{duan2021adversariallaserbeameffective,
  title         = {Adversarial Laser Beam: Effective Physical-World Attack to DNNs in a Blink},
  author        = {Ranjie Duan and Xiaofeng Mao and A. K. Qin and Yun Yang and Yuefeng Chen and Shaokai Ye and Yuan He},
  year          = {2021},
  eprint        = {2103.06504},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG},
  url           = {https://arxiv.org/abs/2103.06504}
}
@misc{cheng2022physicalattackmonoculardepth,
  title         = {Physical Attack on Monocular Depth Estimation with Optimal Adversarial Patches},
  author        = {Zhiyuan Cheng and James Liang and Hongjun Choi and Guanhong Tao and Zhiwen Cao and Dongfang Liu and Xiangyu Zhang},
  year          = {2022},
  eprint        = {2207.04718},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CV},
  url           = {https://arxiv.org/abs/2207.04718}
}
@misc{eykholt2018robustphysicalworldattacksdeep,
  title         = {Robust Physical-World Attacks on Deep Learning Models},
  author        = {Kevin Eykholt and Ivan Evtimov and Earlence Fernandes and Bo Li and Amir Rahmati and Chaowei Xiao and Atul Prakash and Tadayoshi Kohno and Dawn Song},
  year          = {2018},
  eprint        = {1707.08945},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CR},
  url           = {https://arxiv.org/abs/1707.08945}
}
@misc{nguyen2020adversariallightprojectionattacks,
  title         = {Adversarial Light Projection Attacks on Face Recognition Systems: A Feasibility Study},
  author        = {Dinh-Luan Nguyen and Sunpreet S. Arora and Yuhang Wu and Hao Yang},
  year          = {2020},
  eprint        = {2003.11145},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CV},
  url           = {https://arxiv.org/abs/2003.11145}
}
@inproceedings{236234,
  author    = {Ambra Demontis and Marco Melis and Maura Pintor and Matthew Jagielski and Battista Biggio and Alina Oprea and Cristina Nita-Rotaru and Fabio Roli},
  title     = {Why Do Adversarial Attacks Transfer? Explaining Transferability of Evasion and Poisoning Attacks},
  booktitle = {28th USENIX Security Symposium (USENIX Security 19)},
  year      = {2019},
  isbn      = {978-1-939133-06-9},
  address   = {Santa Clara, CA},
  pages     = {321--338},
  url       = {https://www.usenix.org/conference/usenixsecurity19/presentation/demontis},
  publisher = {USENIX Association},
  month     = aug
}
@misc{chattopadhyay2025surveyadversarialdefensesvisionbased,
  title         = {A Survey of Adversarial Defenses in Vision-based Systems: Categorization, Methods and Challenges},
  author        = {Nandish Chattopadhyay and Abdul Basit and Bassem Ouni and Muhammad Shafique},
  year          = {2025},
  eprint        = {2503.00384},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CV},
  url           = {https://arxiv.org/abs/2503.00384}
}
@article{CHEN2024121630,
  title    = {Poisoning QoS-aware cloud API recommender system with generative adversarial network attack},
  journal  = {Expert Systems with Applications},
  volume   = {238},
  pages    = {121630},
  year     = {2024},
  issn     = {0957-4174},
  doi      = {https://doi.org/10.1016/j.eswa.2023.121630},
  url      = {https://www.sciencedirect.com/science/article/pii/S0957417423021322},
  author   = {Zhen Chen and Taiyu Bao and Wenchao Qi and Dianlong You and Linlin Liu and Limin Shen},
  keywords = {Recommender system, Data poisoning attack, Cloud API, Quality of service, Generative adversarial network},
  abstract = {With the proliferation and deepening of service-oriented architecture, more and more enterprises and organizations are exposing their computing functions and big data to the Internet in the form of cloud APIs to support service-oriented software development. This has resulted in a plethora of cloud APIs with similar functionality appearing on the Web, drowning users in a sea of cloud API choices. To solve this problem, quality of service (QoS)-aware recommender system is then widely applied to the selection of cloud APIs. Due to the dynamic and open network environment, the QoS-aware cloud API recommender systems are vulnerable to data poisoning attacks, where attackers inject poisoned data to skew the recommender system and make the recommendation direction follow the attacker's will. Given the lack of data poisoning attack methods and robustness analysis for existing QoS-aware cloud API recommender systems, in this work, we first built a general poisoning attack framework for QoS-aware cloud API recommender systems to elucidate and standardize the attack process. Then, we proposed a deep learning-based poison attack approach, which uses generative adversarial network (GAN) to learn the cloud API QoS data distribution of real users in an adversarial way, so as to generate high-quality fake user attack vectors. We conducted extensive experiments on real-world QoS datasets, and the experimental results show that our proposed GAN-based poisoning attack is effective and can better hide itself from being detected. In addition, we analyzed the data poisoning attack mechanism and the robustness of the cloud API recommender system based on four categories of twelve recommendation methods, thereby raising awareness about the security of cloud API recommendation and helping the recommender system defenders to develop more targeted defense strategies.}
}
@misc{wang2024poisoningattacksdefensesrecommender,
  title         = {Poisoning Attacks and Defenses in Recommender Systems: A Survey},
  author        = {Zongwei Wang and Junliang Yu and Min Gao and Wei Yuan and Guanhua Ye and Shazia Sadiq and Hongzhi Yin},
  year          = {2024},
  eprint        = {2406.01022},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CR},
  url           = {https://arxiv.org/abs/2406.01022}
}
@misc{zhao2025datapoisoningdeeplearning,
  title         = {Data Poisoning in Deep Learning: A Survey},
  author        = {Pinlong Zhao and Weiyao Zhu and Pengfei Jiao and Di Gao and Ou Wu},
  year          = {2025},
  eprint        = {2503.22759},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CR},
  url           = {https://arxiv.org/abs/2503.22759}
}
@misc{villani2024sonicfasttransferabledata,
  title         = {Sonic: Fast and Transferable Data Poisoning on Clustering Algorithms},
  author        = {Francesco Villani and Dario Lazzaro and Antonio Emanuele Cinà and Matteo Dell'Amico and Battista Biggio and Fabio Roli},
  year          = {2024},
  eprint        = {2408.07558},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CR},
  url           = {https://arxiv.org/abs/2408.07558}
}
@misc{cleanLaberPoisoning,
  title         = {Transferable Clean-Label Poisoning Attacks on Deep Neural Nets},
  author        = {Chen Zhu and W. Ronny Huang and Ali Shafahi and Hengduo Li and Gavin Taylor and Christoph Studer and Tom Goldstein},
  year          = {2019},
  eprint        = {1905.05897},
  archiveprefix = {arXiv},
  primaryclass  = {stat.ML},
  url           = {https://arxiv.org/abs/1905.05897}
}
@misc{connors2022machinelearningdetectingmalware,
      title={Machine Learning for Detecting Malware in PE Files}, 
      author={Collin Connors and Dilip Sarkar},
      year={2022},
      eprint={2212.13988},
      archivePrefix={arXiv},
      primaryClass={cs.CR},
      url={https://arxiv.org/abs/2212.13988}, 
}
@misc{924286,
  author={Schultz, M.G. and Eskin, E. and Zadok, F. and Stolfo, S.J.},
  booktitle={Proceedings 2001 IEEE Symposium on Security and Privacy. S&P 2001}, 
  title={Data mining methods for detection of new malicious executables}, 
  year={2001},
  volume={},
  number={},
  pages={38-49},
  keywords={Data mining;Computer science;Data security;Testing;Face detection;Computer security;Information security;Permission;Training data;Protection},
  doi={10.1109/SECPRI.2001.924286}
  }

@article{szegedy2013intriguing,
  title   = {Intriguing properties of neural networks},
  author  = {Szegedy, Christian and Zaremba, Wojciech and Sutskever, Ilya and Bruna, Joan and Erhan, Dumitru and Goodfellow, Ian and Fergus, Rob},
  journal = {arXiv preprint arXiv:1312.6199},
  year    = {2013}
}

@article{app9050909,
  author         = {Qiu, Shilin and Liu, Qihe and Zhou, Shijie and Wu, Chunjiang},
  title          = {Review of Artificial Intelligence Adversarial Attack and Defense Technologies},
  journal        = {Applied Sciences},
  volume         = {9},
  year           = {2019},
  number         = {5},
  article-number = {909},
  url            = {https://www.mdpi.com/2076-3417/9/5/909},
  issn           = {2076-3417},
  abstract       = {In recent years, artificial intelligence technologies have been widely used in computer vision, natural language processing, automatic driving, and other fields. However, artificial intelligence systems are vulnerable to adversarial attacks, which limit the applications of artificial intelligence (AI) technologies in key security fields. Therefore, improving the robustness of AI systems against adversarial attacks has played an increasingly important role in the further development of AI. This paper aims to comprehensively summarize the latest research progress on adversarial attack and defense technologies in deep learning. According to the target model’s different stages where the adversarial attack occurred, this paper expounds the adversarial attack methods in the training stage and testing stage respectively. Then, we sort out the applications of adversarial attack technologies in computer vision, natural language processing, cyberspace security, and the physical world. Finally, we describe the existing adversarial defense methods respectively in three main categories, i.e., modifying data, modifying models and using auxiliary tools.},
  doi            = {10.3390/app9050909}
}
@article{carlini2017towards,
  author     = {Nicholas Carlini and
                David A. Wagner},
  title      = {Towards Evaluating the Robustness of Neural Networks},
  journal    = {CoRR},
  volume     = {abs/1608.04644},
  year       = {2016},
  url        = {http://arxiv.org/abs/1608.04644},
  eprinttype = {arXiv},
  eprint     = {1608.04644},
  timestamp  = {Mon, 13 Aug 2018 16:46:14 +0200},
  biburl     = {https://dblp.org/rec/journals/corr/CarliniW16a.bib},
  bibsource  = {dblp computer science bibliography, https://dblp.org}
}


@techreport{nist_ai100_2_2025,
  author      = {Vassilev, Apostol and Oprea, Alina and Fordyce, Alie and Anderson, Hyrum and Davies, Xander and Hamin, Maia},
  title       = {Adversarial Machine Learning: A Taxonomy and Terminology of Attacks and Mitigations},
  institution = {National Institute of Standards and Technology},
  year        = {2025},
  number      = {NIST AI 100-2e2025},
  doi         = {10.6028/NIST.AI.100-2e2025},
  url         = {https://doi.org/10.6028/NIST.AI.100-2e2025}
}
@misc{li2021adversarialattacklargescale,
  title         = {Adversarial Attack on Large Scale Graph},
  author        = {Jintang Li and Tao Xie and Liang Chen and Fenfang Xie and Xiangnan He and Zibin Zheng},
  year          = {2021},
  eprint        = {2009.03488},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG},
  url           = {https://arxiv.org/abs/2009.03488}
}
@article{9238430,
  author   = {Chen, Sizhe and He, Zhengbao and Sun, Chengjin and Yang, Jie and Huang, Xiaolin},
  journal  = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  title    = {Universal Adversarial Attack on Attention and the Resulting Dataset DAmageNet},
  year     = {2022},
  volume   = {44},
  number   = {4},
  pages    = {2188-2197},
  keywords = {Heating systems;Training;Neural networks;Perturbation methods;Semantics;Visualization;Error analysis;Adversarial attack;attention;transferability;black-box attack;DAmageNet},
  doi      = {10.1109/TPAMI.2020.3033291}
}
@misc{luo2022frequencydrivenimperceptibleadversarialattack,
  title         = {Frequency-driven Imperceptible Adversarial Attack on Semantic Similarity},
  author        = {Cheng Luo and Qinliang Lin and Weicheng Xie and Bizhu Wu and Jinheng Xie and Linlin Shen},
  year          = {2022},
  eprint        = {2203.05151},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CV},
  url           = {https://arxiv.org/abs/2203.05151}
}
@misc{papernot2017practicalblackboxattacksmachine,
  title         = {Practical Black-Box Attacks against Machine Learning},
  author        = {Nicolas Papernot and Patrick McDaniel and Ian Goodfellow and Somesh Jha and Z. Berkay Celik and Ananthram Swami},
  year          = {2017},
  eprint        = {1602.02697},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CR},
  url           = {https://arxiv.org/abs/1602.02697}
}
@misc{geirhos2022imagenettrainedcnnsbiasedtexture,
  title         = {ImageNet-trained CNNs are biased towards texture; increasing shape bias improves accuracy and robustness},
  author        = {Robert Geirhos and Patricia Rubisch and Claudio Michaelis and Matthias Bethge and Felix A. Wichmann and Wieland Brendel},
  year          = {2022},
  eprint        = {1811.12231},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CV},
  url           = {https://arxiv.org/abs/1811.12231}
}


@misc{dong2018boostingadversarialattacksmomentum,
  title         = {Boosting Adversarial Attacks with Momentum},
  author        = {Yinpeng Dong and Fangzhou Liao and Tianyu Pang and Hang Su and Jun Zhu and Xiaolin Hu and Jianguo Li},
  year          = {2018},
  eprint        = {1710.06081},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG},
  url           = {https://arxiv.org/abs/1710.06081}
}
@inproceedings{gutierrez-megias-etal-2024-smart,
  title     = {Smart Lexical Search for Label Flipping Adversial Attack},
  author    = {Guti{\'e}rrez-Meg{\'i}as, Alberto  and
               Jim{\'e}nez-Zafra, Salud Mar{\'i}a  and
               Ure{\~n}a, L. Alfonso  and
               Mart{\'i}nez-C{\'a}mara, Eugenio},
  editor    = {Habernal, Ivan  and
               Ghanavati, Sepideh  and
               Ravichander, Abhilasha  and
               Jain, Vijayanta  and
               Thaine, Patricia  and
               Igamberdiev, Timour  and
               Mireshghallah, Niloofar  and
               Feyisetan, Oluwaseyi},
  booktitle = {Proceedings of the Fifth Workshop on Privacy in Natural Language Processing},
  month     = aug,
  year      = {2024},
  address   = {Bangkok, Thailand},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/2024.privatenlp-1.11/},
  pages     = {97--106},
  abstract  = {Language models are susceptible to vulnerability through adversarial attacks, using manipulations of the input data to disrupt their performance. Accordingly, it represents a cibersecurity leak. Data manipulations are intended to be unidentifiable by the learning model and by humans, small changes can disturb the final label of a classification task. Hence, we propose a novel attack built upon explainability methods to identify the salient lexical units to alter in order to flip the classification label. We asses our proposal on a disinformation dataset, and we show that our attack reaches high balance among stealthiness and efficiency.}
}
@misc{kusaka2025costminimizedlabelflippingpoisoningattack,
  title         = {Cost-Minimized Label-Flipping Poisoning Attack to LLM Alignment},
  author        = {Shigeki Kusaka and Keita Saito and Mikoto Kudo and Takumi Tanabe and Akifumi Wachi and Youhei Akimoto},
  year          = {2025},
  eprint        = {2511.09105},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG},
  url           = {https://arxiv.org/abs/2511.09105}
}@misc{aryal2023analysislabelflippoisoningattack,
  title         = {Analysis of Label-Flip Poisoning Attack on Machine Learning Based Malware Detector},
  author        = {Kshitiz Aryal and Maanak Gupta and Mahmoud Abdelsalam},
  year          = {2023},
  eprint        = {2301.01044},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CR},
  url           = {https://arxiv.org/abs/2301.01044}
}
@article{GIBERT2020102526,
  title    = {The rise of machine learning for detection and classification of malware: Research developments, trends and challenges},
  journal  = {Journal of Network and Computer Applications},
  volume   = {153},
  pages    = {102526},
  year     = {2020},
  issn     = {1084-8045},
  doi      = {https://doi.org/10.1016/j.jnca.2019.102526},
  url      = {https://www.sciencedirect.com/science/article/pii/S1084804519303868},
  author   = {Daniel Gibert and Carles Mateu and Jordi Planes},
  keywords = {Malware detection, Feature engineering, Machine learning, Deep learning, Multimodal learning},
  abstract = {The struggle between security analysts and malware developers is a never-ending battle with the complexity of malware changing as quickly as innovation grows. Current state-of-the-art research focus on the development and application of machine learning techniques for malware detection due to its ability to keep pace with malware evolution. This survey aims at providing a systematic and detailed overview of machine learning techniques for malware detection and in particular, deep learning techniques. The main contributions of the paper are: (1) it provides a complete description of the methods and features in a traditional machine learning workflow for malware detection and classification, (2) it explores the challenges and limitations of traditional machine learning and (3) it analyzes recent trends and developments in the field with special emphasis on deep learning approaches. Furthermore, (4) it presents the research issues and unsolved challenges of the state-of-the-art techniques and (5) it discusses the new directions of research. The survey helps researchers to have an understanding of the malware detection field and of the new developments and directions of research explored by the scientific community to tackle the problem.}
}
@misc{chen2025rethinkingexploringstringbasedmalware,
  title         = {Rethinking and Exploring String-Based Malware Family Classification in the Era of LLMs and RAG},
  author        = {Yufan Chen and Daoyuan Wu and Juantao Zhong and Zicheng Zhang and Debin Gao and Shuai Wang and Yingjiu Li and Ning Liu and Jiachi Chen and Rocky K. C. Chang},
  year          = {2025},
  eprint        = {2507.04055},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CR},
  url           = {https://arxiv.org/abs/2507.04055}
}

@article{electronics9111777,
  author         = {Ali, Muhammad and Shiaeles, Stavros and Bendiab, Gueltoum and Ghita, Bogdan},
  title          = {MALGRA: Machine Learning and N-Gram Malware Feature Extraction and Detection System},
  journal        = {Electronics},
  volume         = {9},
  year           = {2020},
  number         = {11},
  article-number = {1777},
  url            = {https://www.mdpi.com/2079-9292/9/11/1777},
  issn           = {2079-9292},
  abstract       = {Detection and mitigation of modern malware are critical for the normal operation of an organisation. Traditional defence mechanisms are becoming increasingly ineffective due to the techniques used by attackers such as code obfuscation, metamorphism, and polymorphism, which strengthen the resilience of malware. In this context, the development of adaptive, more effective malware detection methods has been identified as an urgent requirement for protecting the IT infrastructure against such threats, and for ensuring security. In this paper, we investigate an alternative method for malware detection that is based on N-grams and machine learning. We use a dynamic analysis technique to extract an Indicator of Compromise (IOC) for malicious files, which are represented using N-grams. The paper also proposes TF-IDF as a novel alternative used to identify the most significant N-grams features for training a machine learning algorithm. Finally, the paper evaluates the proposed technique using various supervised machine-learning algorithms. The results show that Logistic Regression, with a score of 98.4%, provides the best classification accuracy when compared to the other classifiers used.},
  doi            = {10.3390/electronics9111777}
}
@misc{fellicious2025malwaredetectionbasedapi,
  title         = {Malware Detection based on API calls},
  author        = {Christofer Fellicious and Manuel Bischof and Kevin Mayer and Dorian Eikenberg and Stefan Hausotte and Hans P. Reiser and Michael Granitzer},
  year          = {2025},
  eprint        = {2502.12863},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CR},
  url           = {https://arxiv.org/abs/2502.12863}
}
@article{MANIRIHO2023103704,
  title    = {API-MalDetect: Automated malware detection framework for windows based on API calls and deep learning techniques},
  journal  = {Journal of Network and Computer Applications},
  volume   = {218},
  pages    = {103704},
  year     = {2023},
  issn     = {1084-8045},
  doi      = {https://doi.org/10.1016/j.jnca.2023.103704},
  url      = {https://www.sciencedirect.com/science/article/pii/S1084804523001236},
  author   = {Pascal Maniriho and Abdun Naser Mahmood and Mohammad Jabed Morshed Chowdhury},
  keywords = {Malware analysis, Malware detection, Dynamic analysis, Convolutional neural network, API calls, Machine learning, Deep learning},
  abstract = {This paper presents API-MalDetect, a new deep learning-based automated framework for detecting malware attacks in Windows systems. The framework uses an NLP-based encoder for API calls and a hybrid automatic feature extractor based on convolutional neural networks (CNNs) and bidirectional gated recurrent units (BiGRU) to extract features from raw and long sequences of API calls. The proposed framework is designed to detect unseen malware attacks and prevent performance degradation over time or across different rates of exposure to malware by reducing temporal bias and spatial bias during training and testing. Experimental results show that API-MalDetect outperforms existing state-of-the-art malware detection techniques in terms of accuracy, precision, recall, F1-score, and AUC-ROC on different benchmark datasets of API call sequences. These results demonstrate that the ability to automatically identify unique and highly relevant patterns from raw and long sequences of API calls is effective in distinguishing malware attacks from benign activities in Windows systems using the proposed API-MalDetect framework. API-MalDetect is also able to show cybersecurity experts which API calls were most important in malware identification. Furthermore, we make our dataset available to the research community.}
}

@article{Boukhtouta2015NetworkMC,
  title   = {Network malware classification comparison using DPI and flow packet headers},
  author  = {Amine Boukhtouta and Serguei A. Mokhov and Nour-Eddine Lakhdari and Mourad Debbabi and Joey Paquet},
  journal = {Journal of Computer Virology and Hacking Techniques},
  volume  = {12},
  number  = {2},
  pages   = {69--100},
  year    = {2015},
  doi     = {10.1007/s11416-015-0247-x}
}
@inproceedings{nataraj2011malware,
  title     = {Malware images: visualization and automatic classification},
  author    = {Nataraj, Lakshmanan and Karthikeyan, S and Jacob, Gregoire and Manjunath, BS},
  booktitle = {Proceedings of the 8th international symposium on visualization for cyber security},
  pages     = {1--7},
  year      = {2011}
}

@inproceedings{pascanu2015malware,
  title        = {Malware classification with recurrent networks},
  author       = {Pascanu, Razvan and Stokes, Jack W and Sanossian, Hermine and Marinescu, Miltiadis and Thomas, Anil},
  booktitle    = {2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages        = {1916--1920},
  year         = {2015},
  organization = {IEEE}
}

@inproceedings{hassen2017scalable,
  title     = {Scalable function call graph-based malware classification},
  author    = {Hassen, Mehadi and Chan, Philip K},
  booktitle = {Proceedings of the 7th ACM on Conference on Data and Application Security and Privacy},
  pages     = {239--248},
  year      = {2017}
}
@misc{raff2017malwaredetectioneatingexe,
  title         = {Malware Detection by Eating a Whole EXE},
  author        = {Edward Raff and Jon Barker and Jared Sylvester and Robert Brandon and Bryan Catanzaro and Charles Nicholas},
  year          = {2017},
  eprint        = {1710.09435},
  archiveprefix = {arXiv},
  primaryclass  = {stat.ML},
  url           = {https://arxiv.org/abs/1710.09435}
}

@article{fi16050168,
  author         = {Imran, Muhammad and Appice, Annalisa and Malerba, Donato},
  title          = {Evaluating Realistic Adversarial Attacks against Machine Learning Models for Windows PE Malware Detection},
  journal        = {Future Internet},
  volume         = {16},
  year           = {2024},
  number         = {5},
  article-number = {168},
  url            = {https://www.mdpi.com/1999-5903/16/5/168},
  issn           = {1999-5903},
  abstract       = {During the last decade, the cybersecurity literature has conferred a high-level role to machine learning as a powerful security paradigm to recognise malicious software in modern anti-malware systems. However, a non-negligible limitation of machine learning methods used to train decision models is that adversarial attacks can easily fool them. Adversarial attacks are attack samples produced by carefully manipulating the samples at the test time to violate the model integrity by causing detection mistakes. In this paper, we analyse the performance of five realistic target-based adversarial attacks, namely Extend, Full DOS, Shift, FGSM padding + slack and GAMMA, against two machine learning models, namely MalConv and LGBM, learned to recognise Windows Portable Executable (PE) malware files. Specifically, MalConv is a Convolutional Neural Network (CNN) model learned from the raw bytes of Windows PE files. LGBM is a Gradient-Boosted Decision Tree model that is learned from features extracted through the static analysis of Windows PE files. Notably, the attack methods and machine learning models considered in this study are state-of-the-art methods broadly used in the machine learning literature for Windows PE malware detection tasks. In addition, we explore the effect of accounting for adversarial attacks on securing machine learning models through the adversarial training strategy. Therefore, the main contributions of this article are as follows: (1) We extend existing machine learning studies that commonly consider small datasets to explore the evasion ability of state-of-the-art Windows PE attack methods by increasing the size of the evaluation dataset. (2) To the best of our knowledge, we are the first to carry out an exploratory study to explain how the considered adversarial attack methods change Windows PE malware to fool an effective decision model. (3) We explore the performance of the adversarial training strategy as a means to secure effective decision models against adversarial Windows PE malware files generated with the considered attack methods. Hence, the study explains how GAMMA can actually be considered the most effective evasion method for the performed comparative analysis. On the other hand, the study shows that the adversarial training strategy can actually help in recognising adversarial PE malware generated with GAMMA by also explaining how it changes model decisions.},
  doi            = {10.3390/fi16050168}
}

@article{Bartlett_2020,
  title     = {Benign overfitting in linear regression},
  volume    = {117},
  issn      = {1091-6490},
  url       = {http://dx.doi.org/10.1073/pnas.1907378117},
  doi       = {10.1073/pnas.1907378117},
  number    = {48},
  journal   = {Proceedings of the National Academy of Sciences},
  publisher = {Proceedings of the National Academy of Sciences},
  author    = {Bartlett, Peter L. and Long, Philip M. and Lugosi, Gábor and Tsigler, Alexander},
  year      = {2020},
  month     = apr,
  pages     = {30063–30070}
}

@inproceedings{Lobascio2025Adversarial,
  author    = {Luca Lobascio and Giuseppina Andresini and Annalisa Appice and Donato Malerba},
  title     = {Adversarial Training to Improve Accuracy and Robustness of a Windows PE Malware Detection Model},
  booktitle = {Proceedings of the Joint National Conference on Cybersecurity (ITASEC \& SERICS 2025)},
  year      = {2025},
  series    = {CEUR Workshop Proceedings},
  volume    = {3962},
  publisher = {CEUR-WS.org},
  address   = {Bologna, Italy},
  url       = {https://ceur-ws.org/Vol-3962/paper2.pdf}
}