\chapter{Conclusioni e Contributi}

Questo lavoro ha esplorato la vulnerabilità dei classificatori di malware basati su machine learning agli attacchi di poisoning, concentrandosi su uno scenario che vede la trasformazione di file legittimi (Goodware) in campioni classificati erroneamente come Malware. Tale prospettiva, inversa rispetto alla letteratura tradizionale, consente di valutare la robustezza dei sistemi di rilevamento rispetto a falsi positivi, un problema che può minare gravemente la fiducia degli utenti nei sistemi anti-malware.


Il confronto tra esempi adversarial standard e quelli filtrati tramite VirusTotal ha confermato che l'efficacia del poisoning persiste anche quando si utilizzano solo campioni riconosciuti come malevoli nel mondo reale. Sebbene la riduzione del numero di esempi iniettati comporti una lieve diminuzione dell'impatto rispetto all'attacco standard, l'effetto di degradazione delle performance rimane osservabile rispetto al modello originale. Ciò indica che la minaccia è concreta anche in scenari operativi in cui gli esempi adversarial sono soggetti a validazione esterna.

I principali contributi di questa tesi si articolano su due fronti complementari. In primo luogo, è stato condotto uno studio sistematico dell'effetto di poisoning in uno scenario che applica attacchi a file Goodware per indurre classificazioni errate come Malware. Questo approccio ha permesso di valutare la robustezza dei classificatori rispetto a falsi positivi e di analizzare come diverse strategie di poisoning (label flipping e clean label, con iniezione o rimpiazzo) influenzino le performance dei modelli in presenza di bilanciamento o sbilanciamento del dataset. In particolare, i risultati sperimentali hanno evidenziato che l'approccio di \textbf{Label Flipping} si è rivelato l'unico effettivamente capace di compromettere le prestazioni del classificatore, mentre le strategie Clean Label sono risultate inefficaci, agendo in alcuni casi come un meccanismo di rafforzamento del modello. In secondo luogo, è stata verificata la transferibilità degli esempi adversarial, valutando la capacità degli esempi generati da modelli surrogati (MalConv e \(DNN_{ember}\)) di trasferirsi efficacemente al modello target (\(DNN_{target}\)) addestrato su un dataset differente (WIPE4ADV). L'analisi ha evidenziato una significativa variabilità nell'efficacia degli attacchi in funzione dell'architettura del modello surrogato e dell'algoritmo utilizzato, fornendo indicazioni sulla generalizzabilità delle perturbazioni adversarial tra modelli con caratteristiche diverse. Un aspetto fondamentale della metodologia adottata è stato l'uso di algoritmi di adversarial learning black-box in grado di creare esempi adversarial eseguibili, garantendo così il realismo delle minacce simulate.

I risultati ottenuti aprono diverse direzioni di ricerca future. Sarebbe interessante sviluppare tecniche di poisoning ottimizzate specificamente per la feature collision. Questo approccio mira a generare campioni di avvelenamento che, pur mantenendo la loro etichetta originale, possiedono una rappresentazione nello spazio delle feature quasi indistinguibile da quella dei campioni target che si intende far classificare erroneamente. Poiché gli algoritmi attuali non sono progettati per indurre esplicitamente tali collisioni nello spazio latente, lo sviluppo di tecniche dedicate potrebbe incrementare notevolmente l'efficacia degli attacchi clean label, rendendo la minaccia molto più insidiosa per i sistemi di rilevamento. Parallelamente, è necessario sviluppare strategie di difesa specifiche per contrastare gli attacchi di poisoning che mirano a generare falsi positivi, preservando la fiducia degli utenti nei sistemi anti-malware. Estendere lo studio a modelli con architetture radicalmente diverse, come transformer o modelli ibridi, potrebbe fornire ulteriori insight sulla generalizzabilità degli esempi adversarial e sulla robustezza intrinseca di diverse famiglie di modelli. Un limite riscontrato in questo lavoro è stato l'utilizzo di un dataset di dimensioni contenute. Di conseguenza, uno sviluppo futuro fondamentale prevede l'estensione dell'analisi su dataset più ampi e aggiornati, rappresentativi delle minacce attuali, per valutare la scalabilità e la rilevanza pratica dei risultati in scenari operativi reali su larga scala. 